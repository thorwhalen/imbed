{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- [Streamlining for-search data prep](https://github.com/thorwhalen/imbed/discussions/5)\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from functools import partial, cached_property\n",
    "from dataclasses import dataclass\n",
    "from typing import Mapping, Callable, MutableMapping\n",
    "\n",
    "class Imbed:\n",
    "    docs: Mapping = None\n",
    "    segments: MutableMapping = None\n",
    "    embedder: Callable = None\n",
    "\n",
    "raw_docs = mk_text_store(doc_src_uri)  # the store used will depend on the source and format of where the docs are stored\n",
    "segments = mk_segments_store(raw_docs, ...)  # will not copy any data over, but will give a key-value view of chunked (split) docs\n",
    "search_ctrl = mk_search_controller(vectorDB, embedder, ...)\n",
    "search_ctrl.fit(segments, doc_src_uri, ...)\n",
    "search_ctrl.save(...)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/var/folders/mc/c070wfh51kxd9lft8dl74q1r0000gn/T/tmp09c4oaun'...\n",
      "Cloning into '/var/folders/mc/c070wfh51kxd9lft8dl74q1r0000gn/T/tmp86ib1h_7'...\n",
      "remote: Repository not found.\n",
      "fatal: repository 'https://github.com/thorwhalen/imbed.wiki.git/' not found\n",
      "/Users/thorwhalen/Dropbox/py/proj/t/hubcap/hubcap/util.py:280: UserWarning: It's possible that the repository doesn't have a wiki. Error: Command 'git clone https://github.com/thorwhalen/imbed.wiki.git /var/folders/mc/c070wfh51kxd9lft8dl74q1r0000gn/T/tmp86ib1h_7' returned non-zero exit status 128.\n",
      "  warn(f\"It's possible that the repository doesn't have a wiki. Error: {e}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83110"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hubcap import repo_text_aggregate\n",
    "from pathlib import Path\n",
    "\n",
    "string = repo_text_aggregate('thorwhalen/imbed')\n",
    "Path('/Users/thorwhalen/Dropbox/_odata/ai_contexts/i2i/imbed.md').write_text(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "91.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Iterable\n",
    "\n",
    "\n",
    "def get_kv_items(obj, args=(), kwargs=()):\n",
    "    \"\"\"Generate key-value pairs from an object.\n",
    "    \n",
    "    The object can be a mapping (like a dictionary) or an iterable (like a list).\n",
    "    If the object is a mapping, the key-value pairs are yielded from the items\n",
    "    of the mapping. If the object is an iterable, the key-value pairs are yielded\n",
    "    from the elements of the iterable, with the keys being the indices of the\n",
    "    elements.\n",
    "\n",
    "    \"\"\"\n",
    "    kwargs = dict(kwargs)\n",
    "    if isinstance(obj, Mapping):\n",
    "        yield from obj.items()\n",
    "    elif isinstance(obj, Iterable):\n",
    "        yield from enumerate(obj, *args, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Object of type {type(obj)} is not supported, like your {obj=}.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A base Dacc for imbed pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imbed.base import ImbedBase\n",
    "from typing import Callable, T\n",
    "from imbed.base import TextMapping, MetadataMapping, SegmentMapping, VectorMapping, PlanarVectorMapping\n",
    "from dol import cache_this\n",
    "\n",
    "def identity(x: T) -> T:\n",
    "    return x\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "cache_result = partial(cache_this, cache='cache')\n",
    "\n",
    "class ImbedDaccBase:\n",
    "    cache = {}\n",
    "\n",
    "    text_to_segment: Callable[[TextMapping], SegmentMapping] = identity\n",
    "    segment_to_vector: Callable[[SegmentMapping], VectorMapping] = None\n",
    "    vector_to_planar_vector: Callable[[VectorMapping], PlanarVectorMapping] = None\n",
    "\n",
    "    def download_data(self, uri):\n",
    "        \"\"\"Initial download of data from the source\"\"\"\n",
    "\n",
    "    @cache_result\n",
    "    def texts(self):\n",
    "        \"\"\"key-value view (i.e. Mapping) of the text data\"\"\"\n",
    "        \n",
    "    @cache_result\n",
    "    def text_metadatas(self):\n",
    "        \"\"\"Mapping of the metadata of the text data.\n",
    "\n",
    "        The keys of texts and text_metadatas mappings should be the same\n",
    "        \"\"\"\n",
    "\n",
    "    @cache_result\n",
    "    def text_segments(self):\n",
    "        \"\"\"Mapping of the segments of text data.\n",
    "\n",
    "        Could be computed on the fly from the text_store and a segmentation algorithm, \n",
    "        or precomputed and stored in a separate key-value store.\n",
    "\n",
    "        Preferably, the key of the text store should be able to be computed from key \n",
    "        of the text_segments store, and even contain the information necessary to \n",
    "        extract the segment from the corresponding text store value.\n",
    "\n",
    "        Note that the imbed.segmentation.SegmentMapping class can be used to\n",
    "        create a mapping between the text store and the text segments store.\n",
    "        \"\"\"\n",
    "        return self.text_to_segment(self.texts)\n",
    "\n",
    "\n",
    "    @cache_result\n",
    "    def segment_vectors(self):\n",
    "        \"\"\"Mapping of the vectors (embeddings) of the segments of text data.\n",
    "\n",
    "        The keys of the segment_vectors store should be the same as the keys of the \n",
    "        text_segments store.\n",
    "\n",
    "        Could be computed on the fly from the text_segments and a vectorization algorithm, \n",
    "        or precomputed and stored in a separate key-value store.\n",
    "\n",
    "        Preferably, the key of the text_segments store should be able to be computed from key \n",
    "        of the segment_vectors store, and even contain the information necessary to \n",
    "        extract the segment from the corresponding text segments store value.\n",
    "\n",
    "        Note that the imbed.vectorization.VectorMapping class can be used to\n",
    "        create a mapping between the text segments store and the segment_vectors store.\n",
    "        \"\"\"\n",
    "        \n",
    "    @cache_result\n",
    "    def segment_2d_vectors(self):\n",
    "        \"\"\"Mapping of the 2D vectors of the segments of text data.\n",
    "\n",
    "        The keys of the segment_2d_vectors store should be the same as the keys of the \n",
    "        segment_vectors store.\n",
    "\n",
    "        Could be computed on the fly from the segment_vectors and a dimensionality reduction algorithm, \n",
    "        or precomputed and stored in a separate key-value store.\n",
    "\n",
    "        Preferably, the key of the segment_vectors store should be able to be computed from key \n",
    "        of the segment_2d_vectors store, and even contain the information necessary to \n",
    "        extract the segment from the corresponding segment_vectors store value.\n",
    "\n",
    "        Note that the imbed.dimensionality_reduction.DimensionalityReductionMapping class can be used to\n",
    "        create a mapping between the segment_vectors store and the segment_2d_vectors store.\n",
    "        \"\"\"\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imbed.tests.utils_for_tests import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make and save a markdown file aggregating the imbed repository information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/var/folders/mc/c070wfh51kxd9lft8dl74q1r0000gn/T/tmp5qxw8bb8'...\n",
      "Cloning into '/var/folders/mc/c070wfh51kxd9lft8dl74q1r0000gn/T/tmpp9nq7jmw'...\n",
      "remote: Repository not found.\n",
      "fatal: repository 'https://github.com/thorwhalen/imbed.wiki.git/' not found\n",
      "/Users/thorwhalen/Dropbox/py/proj/t/hubcap/hubcap/util.py:280: UserWarning: It's possible that the repository doesn't have a wiki. Error: Command 'git clone https://github.com/thorwhalen/imbed.wiki.git /var/folders/mc/c070wfh51kxd9lft8dl74q1r0000gn/T/tmpp9nq7jmw' returned non-zero exit status 128.\n",
      "  warn(f\"It's possible that the repository doesn't have a wiki. Error: {e}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "111207"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hubcap import repo_text_aggregate\n",
    "from pathlib import Path\n",
    "\n",
    "Path('~/Downloads/imbed.md').expanduser().write_text(repo_text_aggregate('thorwhalen/imbed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded two different pieces of data so my my first comment would be that it's not really obvious where I have to go to be able to render the data right it's not obvious that it's here I would say we'll talk about that later but maybe this can be in more places people come here to mainly visualise the data so maybe there can be more than one place where they can do that for example where you view the data with the data make a Google Android graph and then maybe it comes over here to go to do that so anyway I find I finally found where are we supposed to render the graph and I would like to mention a few bugs here so first of all here I select planar embeddings and I do render graph and nothing happens and do it again nothing happens now I know now that it's because the ideas required that should be more obvious if there's a problem that it's it gives a message saying hey you need to do this or maybe highlighted or anyway more obvious second of all the I'd really shouldn't be required if there's some rows of data we should be able to do an implicit I'd 0 1 2 3 etc right so I don't think we really should require the ID in fact I already before I found it annoying that I had to create an artificial ID when there wasn't one to be able to render my data so anyway in this case there is this so I don't know if this is implicit one or not if I do that I managed to render the graph of right second bug here I select something else publication you wrap right here nothing is required everything is fine I do render graph and I get the same graph the one from before if I do it again while I can't do it again here but if I come here and select this again then I get this render graph that shows up and I click it again and now I get what I want right if I go back to plan your embeddings and I do render graph nothing happens again yes of course I forgot so somehow it's not symmetrical it seems let me try to do this again I do it once I selected again I do it again maybe if I do this and I say begin let me try this no so again it is symmetrical there is basically some kind of thing happening that I need to resell it for it to work and so that will be all for now oh no I will mention that maybe maybe the data if I find my points right here maybe there can be something here to go to the rendering engine or maybe I render it directly from here anyway I would put a few more points where I can render the data from including maybe selection of the data and then render that that's all\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "recognizer = sr.Recognizer()\n",
    "audio_file_path = \"/Users/thorwhalen/Downloads/cosmograph_review_02_audio.wav\"\n",
    "\n",
    "with sr.AudioFile(audio_file_path) as source:\n",
    "    audio_data = recognizer.record(source)\n",
    "    text = recognizer.recognize_google(audio_data)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"uploaded two different pieces of data so my my first comment would be that it's not really obvious where I have to go to be able to render the data right it's not obvious that it's here I would say we'll talk about that later but maybe this can be in more places people come here to mainly visualise the data so maybe there can be more than one place where they can do that for example where you view the data with the data make a Google Android graph and then maybe it comes over here to go to do that so anyway I find I finally found where are we supposed to render the graph and I would like to mention a few bugs here so first of all here I select planar embeddings and I do render graph and nothing happens and do it again nothing happens now I know now that it's because the ideas required that should be more obvious if there's a problem that it's it gives a message saying hey you need to do this or maybe highlighted or anyway more obvious second of all the I'd really shouldn't be required if there's some rows of data we should be able to do an implicit I'd 0 1 2 3 etc right so I don't think we really should require the ID in fact I already before I found it annoying that I had to create an artificial ID when there wasn't one to be able to render my data so anyway in this case there is this so I don't know if this is implicit one or not if I do that I managed to render the graph of right second bug here I select something else publication you wrap right here nothing is required everything is fine I do render graph and I get the same graph the one from before if I do it again while I can't do it again here but if I come here and select this again then I get this render graph that shows up and I click it again and now I get what I want right if I go back to plan your embeddings and I do render graph nothing happens again yes of course I forgot so somehow it's not symmetrical it seems let me try to do this again I do it once I selected again I do it again maybe if I do this and I say begin let me try this no so again it is symmetrical there is basically some kind of thing happening that I need to resell it for it to work and so that will be all for now oh no I will mention that maybe maybe the data if I find my points right here maybe there can be something here to go to the rendering engine or maybe I render it directly from here anyway I would put a few more points where I can render the data from including maybe selection of the data and then render that that's all\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import attrgetter\n",
    "from dol import Pipe, written_key\n",
    "from oa import prompt_function\n",
    "\n",
    "\n",
    "def video_to_audio(video_file_path):\n",
    "    import moviepy.editor as mp\n",
    "\n",
    "    video = mp.VideoFileClip(video_file_path)\n",
    "    return video\n",
    "\n",
    "\n",
    "def audio_file_to_text(audio_file_path):\n",
    "    import speech_recognition as sr\n",
    "\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_file_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "\n",
    "    return text\n",
    "\n",
    "audio_obj_writer = lambda audio, path: audio.write_audiofile(path)\n",
    "save_audio_to_wav_file = written_key(writer=audio_obj_writer, key='*.wav')\n",
    "video_to_text = Pipe(\n",
    "    video_to_audio, attrgetter('audio'), save_audio_to_wav_file, audio_file_to_text\n",
    ")\n",
    "\n",
    "text_to_dev_takeaways = prompt_function(\n",
    "    \"\"\"\n",
    "From the following transcription, I'd like you to extract \n",
    "the tasks that would be relevant to developers.\n",
    "These tasks should be actionable. They might be bugs to repair or features to add or enhance.\n",
    "I will be copy pasting these as issues in a github repo, so please include a \n",
    "short title and a description for each task, as a bullet point list \n",
    "where the title is bolded and the description comes after the colon. Like this:\n",
    "- **Title 1**: Description of the task 1\n",
    "- **Title 2**: Description of the task 2           \n",
    "\n",
    "Here's my transcription:\n",
    "\n",
    "{text}  \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "audio_to_dev_takeaways = Pipe(audio_file_to_text, text_to_dev_takeaways)\n",
    "video_to_dev_takeaways = Pipe(video_to_text, text_to_dev_takeaways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in /var/folders/mc/c070wfh51kxd9lft8dl74q1r0000gn/T/tmp2eejt0bv.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "video_path = '/Users/thorwhalen/Dropbox/Misc/2024_misc/cosmograph_app_review_01.mov'\n",
    "\n",
    "text = video_to_text(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Pagination Flexibility**: Assess the flexibility of the table pagination feature. There may be a need for different types of navigation between pages, depending on user feedback and needs.\n",
      "- **Filter Functionality**: Investigate the filter function. Currently, it appears to be disabled or not displaying any results. A user should be able to see and select filtering options.\n",
      "- **Infinite Scroll Feasibility**: Consider implementing an infinite scroll feature. This feature could enhance the navigation experience, providing automatic paging for the user.\n",
      "- **Selection Actions**: Check if there are existing actions associated with selection within the interface. If not currently present, ensure the design accommodates for the addition of selection-oriented actions in future.\n",
      "- **Vue Migration Issue**: Review the Vue migration process as it seems not to retain focus on the current view when an action is initiated.\n",
      "- **Ordering and Representation in Scatter Plots**: Reevaluate the approach to representation within scatter plots. Current color-based representation might lead to over or under-representation of certain data points. Consider how colors can be ordered or layered to provide an accurate representation.\n"
     ]
    }
   ],
   "source": [
    "print(text_to_dev_takeaways(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "f = '/Users/thorwhalen/Dropbox/_odata/figiri/github-repos.parquet'\n",
    "df = pd.read_parquet(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3274587, 26)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "owner                                                                  xdedzl\n",
       "name                                                     RuntimeTerrainEditor\n",
       "stars                                                                      82\n",
       "forks                                                                      29\n",
       "watchers                                                                    6\n",
       "isFork                                                                  False\n",
       "isArchived                                                              False\n",
       "languages                                         C#: 162842, ShaderLab: 1965\n",
       "languageCount                                                               2\n",
       "topics                                                                       \n",
       "topicCount                                                                  0\n",
       "diskUsageKb                                                            140632\n",
       "pullRequests                                                                0\n",
       "issues                                                                      0\n",
       "description                                                          运行时地形编辑器\n",
       "primaryLanguage                                                            C#\n",
       "createdAt                                                2019-03-18T15:54:07Z\n",
       "pushedAt                                                 2020-07-29T15:35:12Z\n",
       "defaultBranchCommitCount                                                 40.0\n",
       "license                                                                  None\n",
       "assignableUserCount                                                         1\n",
       "codeOfConduct                                                            None\n",
       "forkingAllowed                                                           True\n",
       "nameWithOwner                                     xdedzl/RuntimeTerrainEditor\n",
       "parent                                                                    NaN\n",
       "embedding                   [-0.054284338, 0.13081394, 0.14242873, -0.0702...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
