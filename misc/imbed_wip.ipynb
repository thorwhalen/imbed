{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the work-in-progress (WIP) scrap for the `imbed` project.\n",
    "\n",
    "It is not meant to be run by all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## index and search docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = {\n",
    "    \"doc_1\": \"Socrates is a man\",\n",
    "    \"doc_2\": \"All men are mortal\",\n",
    "    \"doc_3\": \"Simone de Beauvoir\"\n",
    "}\n",
    "\n",
    "# Make a class VectorDB the wraps a vector database (e.g. chromadb).\n",
    "# It should take as parameters what ever you want (immbeder, text splitter, etc.)\n",
    "# and that has the following methods:\n",
    "vectordb = VectorDB(...)\n",
    "\n",
    "vectordb.add(docs)\n",
    "# should add the docs.values() content, using the docs.keys() as ids\n",
    "# Ids should be maintained to be unique.\n",
    "# If the ids already exist, there should be options on how to handle it:\n",
    "#   - error\n",
    "#   - overwrite\n",
    "#   - skip (i.e. only add the ids that don't already exist)\n",
    "\n",
    "ids: Iterable = vectordb.search(query: str)\n",
    "# if possible, should yield ids instead of returning a list of them\n",
    "# That is, search should return an iterable or iterator.\n",
    "# But do this only if the underlying vectordb search supports it (generators etc.)\n",
    "ids = vectordb.search(query, k=2)  # optionally yeilds the top k results\n",
    "\n",
    "\n",
    "metadata = {\n",
    "    \"doc_1\": {\"author\": \"Plato\", \"year\": 400},\n",
    "    \"doc_2\": {\"author\": \"Plato\", \"year\": 400},\n",
    "    \"doc_3\": {\"author\": \"Sartre\", \"year\": 1949}\n",
    "}\n",
    "vectordb.add_metadata(metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Mapping, Protocol, Any, Literal\n",
    "\n",
    "# Literal is a \"typing\" way of doing Enum\n",
    "WhenIdExists = Literal[\"error\", \"overwrite\", \"skip\"]\n",
    "\n",
    "DocKey = str\n",
    "DocContent = str\n",
    "\n",
    "class VectorDB(Protocol):\n",
    "\n",
    "    def add(self, docs: Mapping[DocKey, DocContent], when_id_exists: WhenIdExists = \"error\"):\n",
    "        pass\n",
    "\n",
    "    def search(self, query: str, k: int = 10) -> Iterable[DocKey]:\n",
    "        pass\n",
    "\n",
    "    def get_doc_content(self, key: DocKey) -> DocContent:\n",
    "        pass\n",
    "\n",
    "    def add_metadata(self, metadata: Mapping[DocKey, dict]):\n",
    "        pass\n",
    "    \n",
    "docs = {\n",
    "    \"doc_1\": \"Socrates is a man\",\n",
    "    \"doc_2\": \"All men are mortal\",\n",
    "    \"doc_3\": \"Simone de Beauvoir\"\n",
    "}\n",
    "\n",
    "def test_mapping_indexing(vectordb: VectorDB):\n",
    "    vectordb.add(docs)\n",
    "    ids = vectordb.search(\"Socrates\")\n",
    "    ids = set(ids)\n",
    "    assert \"doc_1\" in ids\n",
    "\n",
    "    first_id = next(iter(vectordb.search(\"Socrates\")))\n",
    "    assert first_id == 'doc_1'\n",
    "\n",
    "    # test idempotency (look up idempotent)\n",
    "    # i.e. when_id_exists\n",
    "    docs_2 = {\n",
    "        'doc_3': 'de Beauvoir',\n",
    "        'doc_4': 'the second sex'\n",
    "    }\n",
    "    vectordb.add(docs_2, when_id_exists='overwrite')\n",
    "    # test that doc_3 was overwritten and doc_4 was added\n",
    "\n",
    "\n",
    "LangChainVectorDB: VectorDB  # Note I mean LangChainVectorDB has interface of VectorDB\n",
    "ChromaDBVectorDB: VectorDB  # Note I mean LangChainVectorDB has interface of VectorDB\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(a) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m8\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSocrates\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtest_segment_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 37\u001b[0m, in \u001b[0;36mtest_segment_mapping\u001b[0;34m(segment_mapping_cls)\u001b[0m\n\u001b[1;32m     35\u001b[0m a \u001b[38;5;241m=\u001b[39m SegmentMapping(docs, segment_keys)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(a) \u001b[38;5;241m==\u001b[39m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m8\u001b[39m), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m11\u001b[39m)]\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(a) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m a[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m8\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSocrates\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "SegmentKey = Tuple[DocKey, int, int]\n",
    "\n",
    "class SegmentMapping:\n",
    "    def __init__(self, docs: Mapping, segment_keys: List[SegmentKey]):\n",
    "        self.docs = docs\n",
    "        self.segment_keys = segment_keys\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self.segment_keys\n",
    "\n",
    "    def __getitem__(self, key: SegmentKey):\n",
    "        # if you want to validate\n",
    "        # if key not in self.segment_keys:\n",
    "        #     raise KeyError(key)\n",
    "        doc_key, start_idx, end_idx = key\n",
    "        return self.docs[doc_key][start_idx:end_idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.segment_keys) + 1\n",
    "    \n",
    "    def __contains__(self, key: SegmentKey):\n",
    "        # Optional, but faster this way\n",
    "        return key in self.segment_keys\n",
    "    \n",
    "\n",
    "def test_segment_mapping(segment_mapping_cls=SegmentMapping):\n",
    "    docs = {\n",
    "        \"doc_1\": \"Socrates is a man\",\n",
    "        \"doc_2\": \"All men are mortal\",\n",
    "        \"doc_3\": \"Simone de Beauvoir\"\n",
    "    }\n",
    "    segment_keys = [('doc_1', 0, 8), ('doc_1', 9, 11)]\n",
    "    a = SegmentMapping(docs, segment_keys)\n",
    "    assert list(a) == [('doc_1', 0, 8), ('doc_1', 9, 11)]\n",
    "    assert len(a) == 2\n",
    "    assert a['doc_1', 0, 8] == 'Socrates'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(a) == [('doc_1', 0, 8), ('doc_1', 9, 11)]\n",
    "assert a['doc_1', 0, 8] == 'Socrates'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_segment_mapping(segment_mapping_cls=SegmentMapping):\n",
    "    docs = {\n",
    "        \"doc_1\": \"Socrates is a man\",\n",
    "        \"doc_2\": \"All men are mortal\",\n",
    "        \"doc_3\": \"Simone de Beauvoir\"\n",
    "    }\n",
    "    segment_keys = [('doc_1', 0, 8), ('doc_1', 9, 11)]\n",
    "    a = SegmentMapping(docs, segment_keys)\n",
    "    assert list(a) == [('doc_1', 0, 8), ('doc_1', 9, 11)]\n",
    "    assert a['doc_1', 0, 8] == 'Socrates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Socrates'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = {\n",
    "    (\"doc_1\", 0, 8): \"Socrates\",\n",
    "    (\"doc_1\", 10, 12): \"is\",\n",
    "}\n",
    "\n",
    "t[\"doc_1\", 0, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doc_1', 0, 8), ('doc_1', 10, 12)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class Startswith:\n",
    "    def __init__(self, char: str) -> None:\n",
    "        self.char = char\n",
    "    def __contains__(self, key):\n",
    "        return key.startswith(self.char)\n",
    "\n",
    "    \n",
    "s = Startswith('b')\n",
    "\n",
    "'bsdf' in s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Startswith' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m s:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Startswith' object is not iterable"
     ]
    }
   ],
   "source": [
    "for i in s:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/thorwhalen/Dropbox/py/proj/t/imbed/misc'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['srag/srag/mz/mz_mockups.py',\n",
       " 'srag/srag/ad/ad_mockups.py',\n",
       " 'srag/srag/__init__.py',\n",
       " 'srag/srag/tw/__init__.py',\n",
       " 'srag/srag/tw/mockups.py',\n",
       " 'srag/srag/tw/ragdag1.py',\n",
       " 'srag/srag/tw/raglab2.py',\n",
       " 'srag/srag/tw/tw_util.py',\n",
       " 'srag/setup.py',\n",
       " 'raglab/docsrc/conf.py',\n",
       " 'raglab/raglab/util.py',\n",
       " 'raglab/raglab/tests/test_stores_util.py',\n",
       " 'raglab/raglab/__init__.py',\n",
       " 'raglab/raglab/stores/__init__.py',\n",
       " 'raglab/raglab/stores/stores_util.py',\n",
       " 'raglab/raglab/stores/simple_stores.py',\n",
       " 'raglab/raglab/raglab2.py',\n",
       " 'raglab/raglab/retrieval/chroma_client.py',\n",
       " 'raglab/raglab/app.py',\n",
       " 'raglab/raglab/web_services/prompt_templates_dde.py',\n",
       " 'raglab/raglab/web_services/__init__.py',\n",
       " 'raglab/raglab/web_services/mockups.py',\n",
       " 'raglab/raglab/web_services/store_access.py',\n",
       " 'raglab/setup.py']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dol import TextFiles, filt_iter\n",
    "\n",
    "s = TextFiles('/Users/thorwhalen/Dropbox/py/proj/a')\n",
    "s = filt_iter(s, filt=lambda x: x.endswith('.py'))\n",
    "\n",
    "all_py_addaix_files = list(s)\n",
    "print(f\"{len(all_py_addaix_files)}\")\n",
    "list(all_py_addaix_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import re\n",
      "from typing import Mapping\n",
      "\n",
      "import sys\n",
      "\n",
      "# from srag.tw.tw_util import _module_callables, _skip_this\n",
      "\n",
      "\n",
      "# @_skip_this\n",
      "# def igore_this(x):\n",
      "#     return x + 1\n",
      "\n",
      "\n",
      "def user_function(obj):\n",
      "    obj._user_function__ = True\n",
      "    return obj\n",
      "\n",
      "\n",
      "def is_user_function(obj):\n",
      "    return hasattr(obj, \"_user_function__\")\n",
      "\n",
      "\n",
      "def get_user_functions(module):\n",
      "    return [v for k, v in vars(module).items() if is_user_function(v)]\n",
      "\n",
      "\n",
      "datasets = {\n",
      "    'philosophy': {\n",
      "        \"statement_1\": \"Bob is a man\",\n",
      "        \"statement_2\": \"Bob is a name\",\n",
      "        \"statement_3\": \"Men are mortal\",\n",
      "        \"statement_4\": \"1 + 1 = 2\",\n",
      "    },\n",
      "    'fruit': {\n",
      "        \"apples\": \"Apples are red\",\n",
      "        \"more_apples\": \"Apples are fruit\",\n",
      "        \"good\": \"Fruit are good\",\n",
      "        \"outlier\": \"Cars drive on the road\",\n",
      "    },\n",
      "}\n",
      "\n",
      "\n",
      "@user_function\n",
      "def dataset_list():\n",
      "    return list(datasets.keys())\n",
      "\n",
      "\n",
      "stopwords = {\n",
      "    'is',\n",
      "    'are',\n",
      "    'a',\n",
      "    'an',\n",
      "    'the',\n",
      "    'on',\n",
      "    'in',\n",
      "    'at',\n",
      "    'for',\n",
      "    'to',\n",
      "    'of',\n",
      "    'and',\n",
      "    'or',\n",
      "    'not',\n",
      "    'no',\n",
      "    'yes',\n",
      "    'true',\n",
      "    'false',\n",
      "    'good',\n",
      "    'bad',\n",
      "}\n",
      "\n",
      "\n",
      "@user_function\n",
      "def add_stopword(word):\n",
      "    stopwords.add(word.lower())\n",
      "\n",
      "\n",
      "@user_function\n",
      "def get_stopwords():\n",
      "    return list(stopwords)\n",
      "\n",
      "\n",
      "@user_function\n",
      "def search_dataset(\n",
      "    dataset_name: str,\n",
      "    query: str,\n",
      "    # *,\n",
      "    # datasets: Mapping = datasets,\n",
      "    # stopwords: set = stopwords,\n",
      "):\n",
      "    \"\"\"Searches the dataset given by dataset_name for content matching query\"\"\"\n",
      "    query_terms = map(lambda x: x.lower(), re.findall(r'\\w+', query))\n",
      "    query_terms = [x for x in query_terms if x not in stopwords]\n",
      "    query_terms_pattern = re.compile(\n",
      "        '|'.join(f\"({x})\" for x in query_terms), re.IGNORECASE\n",
      "    )\n",
      "\n",
      "    query_matches = lambda x: query_terms_pattern.search(x) is not None\n",
      "\n",
      "    dataset = datasets[dataset_name]\n",
      "\n",
      "    def matching_dataset_ids():\n",
      "        for key, contents in dataset.items():\n",
      "            if query_matches(contents):\n",
      "                yield key\n",
      "\n",
      "    return list(matching_dataset_ids())\n",
      "\n",
      "\n",
      "# _current_module = sys.modules[__name__]\n",
      "\n",
      "# funcs = [dataset_list, add_stopword, get_stopwords, search_dataset]\n",
      "# funcs = [dataset_list, add_stopword, get_stopwords]\n",
      "\n",
      "import sys\n",
      "\n",
      "_current_module = sys.modules[__name__]\n",
      "funcs = get_user_functions(_current_module)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    from py2http import run_app\n",
      "\n",
      "    run_app(funcs, publish_openapi=True, publish_swagger=True)\n",
      "\n",
      "\n",
      "# if __name__ == '__main__':\n",
      "\n",
      "#     from streamlitfront import mk_app\n",
      "\n",
      "#     app = mk_app(funcs)\n",
      "#     app()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(s['srag/srag/ad/ad_mockups.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note `grub`'s interface, which could be used, with some modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SearchStore((store, n_neighbors: int = 10, tokenizer=<function camelcase_and_underscore_tokenizer at 0x137beecb0>))\n",
      "\n",
      "Build a search index for anything (that is given a mapping interface with string values).\n",
      "\n",
      "    A store is anything with a ``collections.Mapping`` interface.\n",
      "\n",
      "\n",
      "TfidfKnnSearcher((search_store: Mapping = <grub.base.DfltSearchStore object at 0x134aaac50>, tfidf: sklearn.feature_extraction.text.TfidfVectorizer = TfidfVectorizer(), knn: sklearn.neighbors._unsupervised.NearestNeighbors = NearestNeighbors(metric='cosine', n_neighbors=10)) -> None)\n",
      "\n",
      "TfidfKnnSearcher(search_store: Mapping = <grub.base.DfltSearchStore object at 0x134aaac50>, tfidf: sklearn.feature_extraction.text.TfidfVectorizer = TfidfVectorizer(), knn: sklearn.neighbors._unsupervised.NearestNeighbors = NearestNeighbors(metric='cosine', n_neighbors=10))\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import grub\n",
    "from i2 import Sig\n",
    "\n",
    "def print_obj_info(obj, max_n_doc_lines=3, *, suffix='\\n'):\n",
    "    sig = Sig(obj)\n",
    "    print(f\"{obj.__name__}({sig})\\n\")\n",
    "    lines = obj.__doc__.splitlines()[:max_n_doc_lines]\n",
    "    docstr = \"\\n\".join(lines)\n",
    "    print(f\"{docstr}\")\n",
    "    print(suffix)\n",
    "\n",
    "print_obj_info(grub.SearchStore)\n",
    "print_obj_info(grub.TfidfKnnSearcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial, cached_property\n",
    "from dataclasses import dataclass\n",
    "from typing import Mapping, Callable, MutableMapping\n",
    "\n",
    "class Imbed:\n",
    "    docs: Mapping = None\n",
    "    segments: MutableMapping = None\n",
    "    embedder: Callable = None\n",
    "\n",
    "raw_docs = mk_text_store(doc_src_uri)  # the store used will depend on the source and format of where the docs are stored\n",
    "segments = mk_segments_store(raw_docs, ...)  # will not copy any data over, but will give a key-value view of chunked (split) docs\n",
    "search_ctrl = mk_search_controller(vectorDB, embedder, ...)\n",
    "search_ctrl.fit(segments, doc_src_uri, ...)\n",
    "search_ctrl.save(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thor was featured on wheel of fortune all people did for the past 30 minutes was stare at a wheel not stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['reddit_jokes', 'stupidstuff', 'wocka']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ha\n",
    "\n",
    "list(ha.joke_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194553"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes = ha.joke_datasets['reddit_jokes']\n",
    "len(jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'body': 'cuz all they do is Tweet',\n",
       " 'id': '5tyyo2',\n",
       " 'score': 1,\n",
       " 'title': \"If I get a bird I'm naming it Trump\"}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke = jokes[14]\n",
    "joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def reddit_joke_metadata_to_key_and_text(metdata: dict, *, sep='\\n'):\n",
    "    return f\"{metdata['title']}{sep}({metdata['body']})\"\n",
    "\n",
    "\n",
    "def partition(a: Iterable, chk_size: int):\n",
    "    return zip(*([iter(a)] * chk_size))\n",
    "\n",
    "\n",
    "segment = next(partition(jokes, 20))\n",
    "len(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import oa\n",
    "\n",
    "# len(oa.embeddings('alexis et sana'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumul = []\n",
    "for joke in segment:\n",
    "    text = reddit_joke_metadata_to_key_and_text(joke)\n",
    "    vector = oa.embeddings(text)\n",
    "    cumul.append(vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1536)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cumul), len(cumul[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(map(reddit_joke_metadata_to_key_and_text, segment))\n",
    "cumul2 = oa.embeddings(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1536)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cumul2), len(cumul2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [x == y for x, y in zip(cumul, cumul2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array(cumul)\n",
    "aa = np.array(cumul2)\n",
    "\n",
    "np.allclose(a, aa, rtol=1e-4, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'body': 'Someone who lays awake at night wondering if there really is a dog. ',\n",
       " 'id': '5tqvz0',\n",
       " 'score': 93,\n",
       " 'title': 'What do you get when you cross an insomniac, an agnostic and a dyslexic?'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jokes[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterable, Iterator\n",
    "isinstance(g, Iterable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed HCP2 publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['publications-hcp2.tsv']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dol import FilesOfZip\n",
    "\n",
    "\n",
    "zipfile = '/Users/thorwhalen/Downloads/publications-hcp2.tsv.zip'\n",
    "s = FilesOfZip(zipfile)\n",
    "list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354165, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>n_cits</th>\n",
       "      <th>micro_cluster</th>\n",
       "      <th>main_field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245658</td>\n",
       "      <td>Standardized low-resolution brain electromagne...</td>\n",
       "      <td>PubMed</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>1056</td>\n",
       "      <td>241</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332595</td>\n",
       "      <td>Electrokinetic migration across artificial liq...</td>\n",
       "      <td>Journal of Chromatography A</td>\n",
       "      <td>2006-03-01</td>\n",
       "      <td>570</td>\n",
       "      <td>180</td>\n",
       "      <td>Physical sciences and engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>718838</td>\n",
       "      <td>Pathophysiology of obsessiveï¿½compulsive diso...</td>\n",
       "      <td>Progress in Neurobiology</td>\n",
       "      <td>2004-02-01</td>\n",
       "      <td>304</td>\n",
       "      <td>1123</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>804345</td>\n",
       "      <td>Der Kinder- und Jugendgesundheitssurvey (KiGGS...</td>\n",
       "      <td>Bundesgesundheitsblatt - Gesundheitsforschung ...</td>\n",
       "      <td>2007-05-01</td>\n",
       "      <td>358</td>\n",
       "      <td>3022</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>831416</td>\n",
       "      <td>Remote Ischemic Preconditioning Provides Early...</td>\n",
       "      <td>Journal of the American College of Cardiology</td>\n",
       "      <td>2005-08-01</td>\n",
       "      <td>370</td>\n",
       "      <td>1057</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>898828</td>\n",
       "      <td>The Unknown Mechanism of the Overtraining Synd...</td>\n",
       "      <td>Sports Medicine</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>300</td>\n",
       "      <td>157</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>905619</td>\n",
       "      <td>Revisiting Frank-Wolfe: Projection-Free Sparse...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>552</td>\n",
       "      <td>1612</td>\n",
       "      <td>Mathematics and computer science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>947309</td>\n",
       "      <td>GFP and ï¿½-Galactosidase Transformation Vecto...</td>\n",
       "      <td>BioTechniques</td>\n",
       "      <td>2000-10-01</td>\n",
       "      <td>311</td>\n",
       "      <td>268</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1212948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Journal of Polymers and the Environment</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>306</td>\n",
       "      <td>389</td>\n",
       "      <td>Physical sciences and engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1252687</td>\n",
       "      <td>Meigsï¿½ syndrome:</td>\n",
       "      <td>European Journal of Obstetrics &amp; Gynecology an...</td>\n",
       "      <td>2000-10-01</td>\n",
       "      <td>421</td>\n",
       "      <td>550</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0   245658  Standardized low-resolution brain electromagne...   \n",
       "1   332595  Electrokinetic migration across artificial liq...   \n",
       "2   718838  Pathophysiology of obsessiveï¿½compulsive diso...   \n",
       "3   804345  Der Kinder- und Jugendgesundheitssurvey (KiGGS...   \n",
       "4   831416  Remote Ischemic Preconditioning Provides Early...   \n",
       "5   898828  The Unknown Mechanism of the Overtraining Synd...   \n",
       "6   905619  Revisiting Frank-Wolfe: Projection-Free Sparse...   \n",
       "7   947309  GFP and ï¿½-Galactosidase Transformation Vecto...   \n",
       "8  1212948                                                NaN   \n",
       "9  1252687                                 Meigsï¿½ syndrome:   \n",
       "\n",
       "                                              source    pub_date  n_cits  \\\n",
       "0                                             PubMed  2002-01-01    1056   \n",
       "1                        Journal of Chromatography A  2006-03-01     570   \n",
       "2                           Progress in Neurobiology  2004-02-01     304   \n",
       "3  Bundesgesundheitsblatt - Gesundheitsforschung ...  2007-05-01     358   \n",
       "4      Journal of the American College of Cardiology  2005-08-01     370   \n",
       "5                                    Sports Medicine  2002-01-01     300   \n",
       "6                                                NaN  2013-06-16     552   \n",
       "7                                      BioTechniques  2000-10-01     311   \n",
       "8            Journal of Polymers and the Environment  2002-01-01     306   \n",
       "9  European Journal of Obstetrics & Gynecology an...  2000-10-01     421   \n",
       "\n",
       "   micro_cluster                         main_field  \n",
       "0            241     Biomedical and health sciences  \n",
       "1            180  Physical sciences and engineering  \n",
       "2           1123     Biomedical and health sciences  \n",
       "3           3022     Biomedical and health sciences  \n",
       "4           1057     Biomedical and health sciences  \n",
       "5            157     Biomedical and health sciences  \n",
       "6           1612   Mathematics and computer science  \n",
       "7            268     Biomedical and health sciences  \n",
       "8            389  Physical sciences and engineering  \n",
       "9            550     Biomedical and health sciences  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "df = pd.read_csv(io.BytesIO(s['publications-hcp2.tsv']), sep='\\t', encoding='latin-1' ) \n",
    "print(f\"{df.shape}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_store = dict(zip(df['id'], df['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dol import JsonFiles\n",
    "target = JsonFiles('/Users/thorwhalen/tmp/pubs_hcp2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Standardized low-resolution brain electromagnetic tomography (sLORETA): technical details.',\n",
       " 'Electrokinetic migration across artificial liquid membranes',\n",
       " 'Pathophysiology of obsessive�compulsive disorder',\n",
       " 'Der Kinder- und Jugendgesundheitssurvey (KiGGS): Stichprobendesign, Response und Nonresponse-Analyse',\n",
       " 'Remote Ischemic Preconditioning Provides Early and Late Protection Against Endothelial Ischemia-Reperfusion Injury in Humans',\n",
       " 'The Unknown Mechanism of the Overtraining Syndrome',\n",
       " 'Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization',\n",
       " 'GFP and �-Galactosidase Transformation Vectors for Promoter/Enhancer Analysis in Drosophila',\n",
       " nan,\n",
       " 'Meigs� syndrome:')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>n_cits</th>\n",
       "      <th>micro_cluster</th>\n",
       "      <th>main_field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245658</td>\n",
       "      <td>Standardized low-resolution brain electromagne...</td>\n",
       "      <td>PubMed</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>1056</td>\n",
       "      <td>241</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332595</td>\n",
       "      <td>Electrokinetic migration across artificial liq...</td>\n",
       "      <td>Journal of Chromatography A</td>\n",
       "      <td>2006-03-01</td>\n",
       "      <td>570</td>\n",
       "      <td>180</td>\n",
       "      <td>Physical sciences and engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>718838</td>\n",
       "      <td>Pathophysiology of obsessive�compulsive disorder</td>\n",
       "      <td>Progress in Neurobiology</td>\n",
       "      <td>2004-02-01</td>\n",
       "      <td>304</td>\n",
       "      <td>1123</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>804345</td>\n",
       "      <td>Der Kinder- und Jugendgesundheitssurvey (KiGGS...</td>\n",
       "      <td>Bundesgesundheitsblatt - Gesundheitsforschung ...</td>\n",
       "      <td>2007-05-01</td>\n",
       "      <td>358</td>\n",
       "      <td>3022</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>831416</td>\n",
       "      <td>Remote Ischemic Preconditioning Provides Early...</td>\n",
       "      <td>Journal of the American College of Cardiology</td>\n",
       "      <td>2005-08-01</td>\n",
       "      <td>370</td>\n",
       "      <td>1057</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>898828</td>\n",
       "      <td>The Unknown Mechanism of the Overtraining Synd...</td>\n",
       "      <td>Sports Medicine</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>300</td>\n",
       "      <td>157</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>905619</td>\n",
       "      <td>Revisiting Frank-Wolfe: Projection-Free Sparse...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-16</td>\n",
       "      <td>552</td>\n",
       "      <td>1612</td>\n",
       "      <td>Mathematics and computer science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>947309</td>\n",
       "      <td>GFP and �-Galactosidase Transformation Vectors...</td>\n",
       "      <td>BioTechniques</td>\n",
       "      <td>2000-10-01</td>\n",
       "      <td>311</td>\n",
       "      <td>268</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1212948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Journal of Polymers and the Environment</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>306</td>\n",
       "      <td>389</td>\n",
       "      <td>Physical sciences and engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1252687</td>\n",
       "      <td>Meigs� syndrome:</td>\n",
       "      <td>European Journal of Obstetrics &amp; Gynecology an...</td>\n",
       "      <td>2000-10-01</td>\n",
       "      <td>421</td>\n",
       "      <td>550</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1279384</td>\n",
       "      <td>Environmental factors influencing heat stress ...</td>\n",
       "      <td>Journal of Animal Science</td>\n",
       "      <td>2006-03-01</td>\n",
       "      <td>531</td>\n",
       "      <td>2365</td>\n",
       "      <td>Life and earth sciences</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0    245658  Standardized low-resolution brain electromagne...   \n",
       "1    332595  Electrokinetic migration across artificial liq...   \n",
       "2    718838   Pathophysiology of obsessive�compulsive disorder   \n",
       "3    804345  Der Kinder- und Jugendgesundheitssurvey (KiGGS...   \n",
       "4    831416  Remote Ischemic Preconditioning Provides Early...   \n",
       "5    898828  The Unknown Mechanism of the Overtraining Synd...   \n",
       "6    905619  Revisiting Frank-Wolfe: Projection-Free Sparse...   \n",
       "7    947309  GFP and �-Galactosidase Transformation Vectors...   \n",
       "8   1212948                                                NaN   \n",
       "9   1252687                                   Meigs� syndrome:   \n",
       "10  1279384  Environmental factors influencing heat stress ...   \n",
       "\n",
       "                                               source    pub_date  n_cits  \\\n",
       "0                                              PubMed  2002-01-01    1056   \n",
       "1                         Journal of Chromatography A  2006-03-01     570   \n",
       "2                            Progress in Neurobiology  2004-02-01     304   \n",
       "3   Bundesgesundheitsblatt - Gesundheitsforschung ...  2007-05-01     358   \n",
       "4       Journal of the American College of Cardiology  2005-08-01     370   \n",
       "5                                     Sports Medicine  2002-01-01     300   \n",
       "6                                                 NaN  2013-06-16     552   \n",
       "7                                       BioTechniques  2000-10-01     311   \n",
       "8             Journal of Polymers and the Environment  2002-01-01     306   \n",
       "9   European Journal of Obstetrics & Gynecology an...  2000-10-01     421   \n",
       "10                          Journal of Animal Science  2006-03-01     531   \n",
       "\n",
       "    micro_cluster                         main_field  \n",
       "0             241     Biomedical and health sciences  \n",
       "1             180  Physical sciences and engineering  \n",
       "2            1123     Biomedical and health sciences  \n",
       "3            3022     Biomedical and health sciences  \n",
       "4            1057     Biomedical and health sciences  \n",
       "5             157     Biomedical and health sciences  \n",
       "6            1612   Mathematics and computer science  \n",
       "7             268     Biomedical and health sciences  \n",
       "8             389  Physical sciences and engineering  \n",
       "9             550     Biomedical and health sciences  \n",
       "10           2365            Life and earth sciences  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from slang import fixed_step_chunker\n",
    "# from dol import JsonFiles\n",
    "# import oa\n",
    "\n",
    "# target = JsonFiles('/Users/thorwhalen/tmp/pubs_hcp2')\n",
    "\n",
    "# for i, chunk in enumerate(fixed_step_chunker(title_store.items(), 10)):\n",
    "#     ids, titles = zip(*chunk)\n",
    "#     embeddings = oa.embeddings(list(titles))\n",
    "#     target[f\"{i:03.0f}\"].update(dict(zip(ids, embeddings)))\n",
    "#     break\n",
    "\n",
    "# BadRequestError: Error code: 400 - {'error': {'message': \"We could not parse the JSON \n",
    "# body of your request. (HINT: This likely means you aren't using your HTTP library correctly. \n",
    "# The OpenAI API expects a JSON payload, but what was sent was not valid JSON. \n",
    "# If you have trouble figuring out how to fix this, please contact us through our \n",
    "# help center at help.openai.com.)\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oa.embeddings(titles[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Meigs� syndrome:'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[-1]  # 'Meigs� syndrome:'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stored HCP2 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "src_folder = '/Users/thorwhalen/Dropbox/_odata/figiri/hcp'\n",
    "pjoin = lambda *args: os.path.join(src_folder, *args)\n",
    "\n",
    "embeddings_filepath = pjoin('publications-hcp2-embeddings.parquet')\n",
    "info_filepath = pjoin('publications-hcp2.tsv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_hcp2.shape=(352508, 2)\n",
      "info_hcp2.shape=(354165, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "embeddings_hcp2 = pd.read_parquet(filepath)\n",
    "print(f\"{embeddings_hcp2.shape=}\")\n",
    "\n",
    "info_hcp2 = pd.read_csv(info_filepath, sep='\\t', encoding='latin-1')\n",
    "print(f\"{info_hcp2.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245658</td>\n",
       "      <td>[0.013564770109951496, 0.010467253625392914, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332595</td>\n",
       "      <td>[0.02909824252128601, 0.03423166275024414, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>718838</td>\n",
       "      <td>[0.008440978825092316, 0.0035484367981553078, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>804345</td>\n",
       "      <td>[0.01794605143368244, 0.04320104047656059, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>831416</td>\n",
       "      <td>[-0.006916569545865059, -0.017052026465535164,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                          embedding\n",
       "0  245658  [0.013564770109951496, 0.010467253625392914, -...\n",
       "1  332595  [0.02909824252128601, 0.03423166275024414, -0....\n",
       "2  718838  [0.008440978825092316, 0.0035484367981553078, ...\n",
       "3  804345  [0.01794605143368244, 0.04320104047656059, -0....\n",
       "4  831416  [-0.006916569545865059, -0.017052026465535164,..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_hcp2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>n_cits</th>\n",
       "      <th>micro_cluster</th>\n",
       "      <th>main_field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245658</td>\n",
       "      <td>Standardized low-resolution brain electromagne...</td>\n",
       "      <td>PubMed</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>1056</td>\n",
       "      <td>241</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332595</td>\n",
       "      <td>Electrokinetic migration across artificial liq...</td>\n",
       "      <td>Journal of Chromatography A</td>\n",
       "      <td>2006-03-01</td>\n",
       "      <td>570</td>\n",
       "      <td>180</td>\n",
       "      <td>Physical sciences and engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>718838</td>\n",
       "      <td>Pathophysiology of obsessiveâcompulsive diso...</td>\n",
       "      <td>Progress in Neurobiology</td>\n",
       "      <td>2004-02-01</td>\n",
       "      <td>304</td>\n",
       "      <td>1123</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>804345</td>\n",
       "      <td>Der Kinder- und Jugendgesundheitssurvey (KiGGS...</td>\n",
       "      <td>Bundesgesundheitsblatt - Gesundheitsforschung ...</td>\n",
       "      <td>2007-05-01</td>\n",
       "      <td>358</td>\n",
       "      <td>3022</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>831416</td>\n",
       "      <td>Remote Ischemic Preconditioning Provides Early...</td>\n",
       "      <td>Journal of the American College of Cardiology</td>\n",
       "      <td>2005-08-01</td>\n",
       "      <td>370</td>\n",
       "      <td>1057</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              title  \\\n",
       "0  245658  Standardized low-resolution brain electromagne...   \n",
       "1  332595  Electrokinetic migration across artificial liq...   \n",
       "2  718838  Pathophysiology of obsessiveâcompulsive diso...   \n",
       "3  804345  Der Kinder- und Jugendgesundheitssurvey (KiGGS...   \n",
       "4  831416  Remote Ischemic Preconditioning Provides Early...   \n",
       "\n",
       "                                              source    pub_date  n_cits  \\\n",
       "0                                             PubMed  2002-01-01    1056   \n",
       "1                        Journal of Chromatography A  2006-03-01     570   \n",
       "2                           Progress in Neurobiology  2004-02-01     304   \n",
       "3  Bundesgesundheitsblatt - Gesundheitsforschung ...  2007-05-01     358   \n",
       "4      Journal of the American College of Cardiology  2005-08-01     370   \n",
       "\n",
       "   micro_cluster                         main_field  \n",
       "0            241     Biomedical and health sciences  \n",
       "1            180  Physical sciences and engineering  \n",
       "2           1123     Biomedical and health sciences  \n",
       "3           3022     Biomedical and health sciences  \n",
       "4           1057     Biomedical and health sciences  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_hcp2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HCP2 publications -- title citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "src_folder = '/Users/thorwhalen/Dropbox/_odata/figiri/hcp'\n",
    "pjoin = lambda *args: os.path.join(src_folder, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_filepath = pjoin('publications-hcp2-embeddings.parquet')\n",
    "citations_hcp3_src = pjoin('citation-links-hcp3.tsv.zip')\n",
    "info_filepath = pjoin('publications-hcp2.tsv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info_hcp2.shape=(354165, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "embeddings_hcp2 = pd.read_parquet(embeddings_filepath)\n",
    "print(f\"{embeddings_hcp2.shape=}\")\n",
    "\n",
    "citations_hcp3 = pd.read_csv(citations_hcp3_src, sep='\\t')\n",
    "print(f\"{citations_hcp3.shape=}\")\n",
    "\n",
    "info_hcp2 = pd.read_csv(info_filepath, sep='\\t', encoding='latin-1')\n",
    "print(f\"{info_hcp2.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "245658    [0.013564770109951496, 0.010467253625392914, -...\n",
       "332595    [0.02909824252128601, 0.03423166275024414, -0....\n",
       "718838    [0.008440978825092316, 0.0035484367981553078, ...\n",
       "804345    [0.01794605143368244, 0.04320104047656059, -0....\n",
       "831416    [-0.006916569545865059, -0.017052026465535164,...\n",
       "Name: embedding, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_hcp2.head()\n",
    "embeddings_hcp2 = embeddings_hcp2.set_index('id')['embedding']\n",
    "embeddings_hcp2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citing_pub_id</th>\n",
       "      <th>cited_pub_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245658</td>\n",
       "      <td>2013150886</td>\n",
       "      <td>2002-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245658</td>\n",
       "      <td>2138790588</td>\n",
       "      <td>2002-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>245658</td>\n",
       "      <td>2334566563</td>\n",
       "      <td>2002-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>718838</td>\n",
       "      <td>1604916109</td>\n",
       "      <td>2004-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>718838</td>\n",
       "      <td>1965439999</td>\n",
       "      <td>2004-02-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   citing_pub_id  cited_pub_id        date\n",
       "0         245658    2013150886  2002-01-01\n",
       "1         245658    2138790588  2002-01-01\n",
       "2         245658    2334566563  2002-01-01\n",
       "3         718838    1604916109  2004-02-01\n",
       "4         718838    1965439999  2004-02-01"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citations_hcp3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(citations_ids)=350777\n",
      "len(embeddings_ids)=352508\n",
      "len(missing_ids)=1616\n"
     ]
    }
   ],
   "source": [
    "citations_ids = set(citations_hcp3['citing_pub_id']) | set(citations_hcp3['cited_pub_id'])\n",
    "embeddings_ids = set(embeddings_hcp2.index)\n",
    "print(f\"{len(citations_ids)=}\")\n",
    "print(f\"{len(embeddings_ids)=}\")\n",
    "\n",
    "# check if all citation ids are in the embeddings (and collect those that are not if any)\n",
    "missing_ids = citations_ids - embeddings_ids\n",
    "print(f\"{len(missing_ids)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1018"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(citations_hcp3['citing_pub_id'])  - embeddings_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1589"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(citations_hcp3['cited_pub_id'])  - embeddings_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212948,\n",
       " 1950462,\n",
       " 4343100,\n",
       " 4355260,\n",
       " 5908806,\n",
       " 7057950,\n",
       " 9477515,\n",
       " 11897853,\n",
       " 14904768,\n",
       " 16253337]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(missing_ids)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citations_hcp3.shape=(4864876, 3)\n"
     ]
    }
   ],
   "source": [
    "# get a version of citations_hcp3 where neither cited_pub_id nor citations_hcp3 contain any of the missing_ids\n",
    "citations_hcp3 = citations_hcp3[~citations_hcp3['citing_pub_id'].isin(missing_ids)]\n",
    "citations_hcp3 = citations_hcp3[~citations_hcp3['cited_pub_id'].isin(missing_ids)]\n",
    "print(f\"{citations_hcp3.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(citations_ids)=349124\n",
      "len(embeddings_ids)=352508\n",
      "len(missing_ids)=0\n"
     ]
    }
   ],
   "source": [
    "citations_ids = set(citations_hcp3['citing_pub_id']) | set(citations_hcp3['cited_pub_id'])\n",
    "embeddings_ids = set(embeddings_hcp2.index)\n",
    "print(f\"{len(citations_ids)=}\")\n",
    "print(f\"{len(embeddings_ids)=}\")\n",
    "\n",
    "# check if all citation ids are in the embeddings (and collect those that are not if any)\n",
    "missing_ids = citations_ids - embeddings_ids\n",
    "print(f\"{len(missing_ids)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group cited_pub_id by citing_pub_id\n",
    "cited_by = citations_hcp3.groupby('cited_pub_id')['citing_pub_id'].apply(set).apply(list).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1966247273,\n",
       " 1992776590,\n",
       " 2097744687,\n",
       " 2127786259,\n",
       " 2161166547,\n",
       " 2054655287,\n",
       " 2607177050,\n",
       " 2169918686]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = next(iter(cited_by.values()))\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate cited embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Sequence\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "def embeddings_for_ids(ids: Sequence[int], embeddings=embeddings_hcp2):\n",
    "    return np.vstack(embeddings.loc[ids].values)\n",
    "\n",
    "def aggregate_values(d: dict, value_aggregator: Callable, value_func: Callable=embeddings_for_ids):\n",
    "    return {k: value_aggregator(value_func(v)) for k, v in d.items()}\n",
    "\n",
    "mean_aggregator = partial(np.mean, axis=0)\n",
    "\n",
    "# a version of the mean that *might* better reflect the cosine similarity between the vectors\n",
    "def cosine_mean_aggregator(vectors: np.ndarray):\n",
    "    return vectors.mean(axis=0) / np.linalg.norm(vectors.mean(axis=0))\n",
    "\n",
    "# normalized means (where we first normalize each vector before taking the mean)\n",
    "# Normalization is done by dividing each vector by its norm\n",
    "# This is in order to (perhaps) better reflect the cosine similarity between the vectors\n",
    "# Note: What about `vectors.mean(axis=0) / np.linalg.norm(vectors.mean(axis=0))`?\n",
    "def normalized_mean_aggregator(vectors: np.ndarray):\n",
    "    return (vectors / np.linalg.norm(vectors, axis=1)[:, None]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_aggregates = aggregate_values(cited_by, mean_aggregator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536,)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k, v = next(iter(mean_aggregates.items()))\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_mean_aggregates = aggregate_values(cited_by, normalized_mean_aggregator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536,)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k, v = next(iter(normalized_mean_aggregates.items()))\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make planar embeddings of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imbed.util import (\n",
    "    umap_2d_embeddings, \n",
    "    umap_2d_embeddings_df, \n",
    "    two_d_embedding_dict_to_df, \n",
    "    save_df_to_zipped_tsv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planar_mean_embeddings.shape=(332256, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245658</td>\n",
       "      <td>3.129996</td>\n",
       "      <td>8.546703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332595</td>\n",
       "      <td>9.747175</td>\n",
       "      <td>-0.088321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>718838</td>\n",
       "      <td>1.771288</td>\n",
       "      <td>9.130220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>804345</td>\n",
       "      <td>-2.349198</td>\n",
       "      <td>7.234386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>831416</td>\n",
       "      <td>-0.462187</td>\n",
       "      <td>1.156000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id         x         y\n",
       "0  245658  3.129996  8.546703\n",
       "1  332595  9.747175 -0.088321\n",
       "2  718838  1.771288  9.130220\n",
       "3  804345 -2.349198  7.234386\n",
       "4  831416 -0.462187  1.156000"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planar_mean_embeddings = umap_2d_embeddings_df(mean_aggregates, key_col='id')\n",
    "print(f\"{planar_mean_embeddings.shape=}\")\n",
    "planar_mean_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.merge(planar_mean_embeddings, info_hcp2, on='id')\n",
    "save_df_to_zipped_tsv(t, pjoin('planar_cited_mean_embeddings'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planar_normalized_mean_embeddings.shape=(332256, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245658</td>\n",
       "      <td>1.503875</td>\n",
       "      <td>8.726212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332595</td>\n",
       "      <td>7.221537</td>\n",
       "      <td>-0.559328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>718838</td>\n",
       "      <td>0.607667</td>\n",
       "      <td>9.450784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>804345</td>\n",
       "      <td>-1.724925</td>\n",
       "      <td>6.536339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>831416</td>\n",
       "      <td>0.241432</td>\n",
       "      <td>0.249355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id         x         y\n",
       "0  245658  1.503875  8.726212\n",
       "1  332595  7.221537 -0.559328\n",
       "2  718838  0.607667  9.450784\n",
       "3  804345 -1.724925  6.536339\n",
       "4  831416  0.241432  0.249355"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planar_normalized_mean_embeddings = umap_2d_embeddings_df(normalized_mean_aggregates, key_col='id')\n",
    "print(f\"{planar_normalized_mean_embeddings.shape=}\")\n",
    "planar_normalized_mean_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting: planar_cited_normalized_mean_embeddings.tsv\n",
      "\tzip warning: zip file empty\n"
     ]
    }
   ],
   "source": [
    "t = pd.merge(planar_normalized_mean_embeddings, info_hcp2, on='id')\n",
    "save_df_to_zipped_tsv(t, pjoin('planar_cited_normalized_mean_embeddings'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>n_cits</th>\n",
       "      <th>micro_cluster</th>\n",
       "      <th>main_field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245658</td>\n",
       "      <td>1.503875</td>\n",
       "      <td>8.726212</td>\n",
       "      <td>Standardized low-resolution brain electromagne...</td>\n",
       "      <td>PubMed</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>1056</td>\n",
       "      <td>241</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332595</td>\n",
       "      <td>7.221537</td>\n",
       "      <td>-0.559328</td>\n",
       "      <td>Electrokinetic migration across artificial liq...</td>\n",
       "      <td>Journal of Chromatography A</td>\n",
       "      <td>2006-03-01</td>\n",
       "      <td>570</td>\n",
       "      <td>180</td>\n",
       "      <td>Physical sciences and engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>718838</td>\n",
       "      <td>0.607667</td>\n",
       "      <td>9.450784</td>\n",
       "      <td>Pathophysiology of obsessiveâcompulsive diso...</td>\n",
       "      <td>Progress in Neurobiology</td>\n",
       "      <td>2004-02-01</td>\n",
       "      <td>304</td>\n",
       "      <td>1123</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>804345</td>\n",
       "      <td>-1.724925</td>\n",
       "      <td>6.536339</td>\n",
       "      <td>Der Kinder- und Jugendgesundheitssurvey (KiGGS...</td>\n",
       "      <td>Bundesgesundheitsblatt - Gesundheitsforschung ...</td>\n",
       "      <td>2007-05-01</td>\n",
       "      <td>358</td>\n",
       "      <td>3022</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>831416</td>\n",
       "      <td>0.241432</td>\n",
       "      <td>0.249355</td>\n",
       "      <td>Remote Ischemic Preconditioning Provides Early...</td>\n",
       "      <td>Journal of the American College of Cardiology</td>\n",
       "      <td>2005-08-01</td>\n",
       "      <td>370</td>\n",
       "      <td>1057</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id         x         y  \\\n",
       "0  245658  1.503875  8.726212   \n",
       "1  332595  7.221537 -0.559328   \n",
       "2  718838  0.607667  9.450784   \n",
       "3  804345 -1.724925  6.536339   \n",
       "4  831416  0.241432  0.249355   \n",
       "\n",
       "                                               title  \\\n",
       "0  Standardized low-resolution brain electromagne...   \n",
       "1  Electrokinetic migration across artificial liq...   \n",
       "2  Pathophysiology of obsessiveâcompulsive diso...   \n",
       "3  Der Kinder- und Jugendgesundheitssurvey (KiGGS...   \n",
       "4  Remote Ischemic Preconditioning Provides Early...   \n",
       "\n",
       "                                              source    pub_date  n_cits  \\\n",
       "0                                             PubMed  2002-01-01    1056   \n",
       "1                        Journal of Chromatography A  2006-03-01     570   \n",
       "2                           Progress in Neurobiology  2004-02-01     304   \n",
       "3  Bundesgesundheitsblatt - Gesundheitsforschung ...  2007-05-01     358   \n",
       "4      Journal of the American College of Cardiology  2005-08-01     370   \n",
       "\n",
       "   micro_cluster                         main_field  \n",
       "0            241     Biomedical and health sciences  \n",
       "1            180  Physical sciences and engineering  \n",
       "2           1123     Biomedical and health sciences  \n",
       "3           3022     Biomedical and health sciences  \n",
       "4           1057     Biomedical and health sciences  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_filepath = pjoin('planar_cited_mean_embeddings.tsv')\n",
    "# save to tsv file\n",
    "planar_mean_embeddings.to_csv(save_filepath, sep='\\t', index=False)\n",
    "# zip the file\n",
    "from dol.zipfiledol import file_or_folder_to_zip_file\n",
    "file_or_folder_to_zip_file(save_filepath, f'{save_filepath}.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'id' is both an index level and a column label, which is ambiguous.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mc/c070wfh51kxd9lft8dl74q1r0000gn/T/ipykernel_36052/992682883.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msave_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'planar_cited_normalized_mean_embeddings.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# join info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplanar_normalized_mean_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_hcp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# save to tsv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# zip the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         )\n\u001b[1;32m    168\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m                         \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1288\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m                         \u001b[0;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1835\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m         \u001b[0mother_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AXIS_LEN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_label_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_label_or_level_ambiguity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1797\u001b[0m             msg = (\n\u001b[1;32m   1798\u001b[0m                 \u001b[0;34mf\"'{key}' is both {level_article} {level_type} level and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 \u001b[0;34mf\"{label_article} {label_type} label, which is ambiguous.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m             )\n\u001b[0;32m-> 1801\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: 'id' is both an index level and a column label, which is ambiguous."
     ]
    }
   ],
   "source": [
    "save_filepath = pjoin('planar_cited_normalized_mean_embeddings.tsv')\n",
    "# join info\n",
    "t = pd.merge(planar_normalized_mean_embeddings, info_hcp2, left_on='id', right_on='id')\n",
    "# save to tsv file\n",
    "t.to_csv(save_filepath, sep='\\t', index=False)\n",
    "# zip the file\n",
    "from dol.zipfiledol import file_or_folder_to_zip_file\n",
    "file_or_folder_to_zip_file(save_filepath, f'{save_filepath}.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>245658</th>\n",
       "      <td>245658</td>\n",
       "      <td>3.753793</td>\n",
       "      <td>8.701797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332595</th>\n",
       "      <td>332595</td>\n",
       "      <td>-1.841340</td>\n",
       "      <td>11.418116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718838</th>\n",
       "      <td>718838</td>\n",
       "      <td>4.334699</td>\n",
       "      <td>7.352377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804345</th>\n",
       "      <td>804345</td>\n",
       "      <td>6.798362</td>\n",
       "      <td>2.089395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831416</th>\n",
       "      <td>831416</td>\n",
       "      <td>7.846370</td>\n",
       "      <td>8.327367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id         x          y\n",
       "id                                 \n",
       "245658  245658  3.753793   8.701797\n",
       "332595  332595 -1.841340  11.418116\n",
       "718838  718838  4.334699   7.352377\n",
       "804345  804345  6.798362   2.089395\n",
       "831416  831416  7.846370   8.327367"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planar_mean_embeddings = two_d_embedding_dict_to_df(t, key_col='id')\n",
    "planar_mean_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the 2d umap of the mean_aggregates, and save it to a tsv file with id, x, and y\n",
    "import umap\n",
    "import pandas as pd\n",
    "\n",
    "umap_2d = umap.UMAP(n_components=2).fit_transform(list(mean_aggregates.values()))\n",
    "umap_2d_df = pd.DataFrame(umap_2d, index=list(mean_aggregates.keys()), columns=['x', 'y'])\n",
    "umap_2d_df.to_csv('/Users/thorwhalen/tmp/umap_2d.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, Dict, KT, List, Sequence\n",
    "\n",
    "def umap_2d_embeddings(kd_embeddings: Mapping[KT, Sequence]) -> Dict[KT, List[float, float]]:\n",
    "    \"\"\"A function that takes a mapping of kd embeddings and returns a dict of the 2d umap embeddings\"\"\"\n",
    "    umap_embeddings = umap.UMAP(n_components=2).fit_transform(list(kd_embeddings.values()))\n",
    "    return {k: list(v) for k, v in zip(kd_embeddings.keys(), umap_embeddings)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>n_cits</th>\n",
       "      <th>micro_cluster</th>\n",
       "      <th>main_field</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245658</td>\n",
       "      <td>Standardized low-resolution brain electromagne...</td>\n",
       "      <td>PubMed</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>1056</td>\n",
       "      <td>241</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332595</td>\n",
       "      <td>Electrokinetic migration across artificial liq...</td>\n",
       "      <td>Journal of Chromatography A</td>\n",
       "      <td>2006-03-01</td>\n",
       "      <td>570</td>\n",
       "      <td>180</td>\n",
       "      <td>Physical sciences and engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>718838</td>\n",
       "      <td>Pathophysiology of obsessiveâcompulsive diso...</td>\n",
       "      <td>Progress in Neurobiology</td>\n",
       "      <td>2004-02-01</td>\n",
       "      <td>304</td>\n",
       "      <td>1123</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>804345</td>\n",
       "      <td>Der Kinder- und Jugendgesundheitssurvey (KiGGS...</td>\n",
       "      <td>Bundesgesundheitsblatt - Gesundheitsforschung ...</td>\n",
       "      <td>2007-05-01</td>\n",
       "      <td>358</td>\n",
       "      <td>3022</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>831416</td>\n",
       "      <td>Remote Ischemic Preconditioning Provides Early...</td>\n",
       "      <td>Journal of the American College of Cardiology</td>\n",
       "      <td>2005-08-01</td>\n",
       "      <td>370</td>\n",
       "      <td>1057</td>\n",
       "      <td>Biomedical and health sciences</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              title  \\\n",
       "0  245658  Standardized low-resolution brain electromagne...   \n",
       "1  332595  Electrokinetic migration across artificial liq...   \n",
       "2  718838  Pathophysiology of obsessiveâcompulsive diso...   \n",
       "3  804345  Der Kinder- und Jugendgesundheitssurvey (KiGGS...   \n",
       "4  831416  Remote Ischemic Preconditioning Provides Early...   \n",
       "\n",
       "                                              source    pub_date  n_cits  \\\n",
       "0                                             PubMed  2002-01-01    1056   \n",
       "1                        Journal of Chromatography A  2006-03-01     570   \n",
       "2                           Progress in Neurobiology  2004-02-01     304   \n",
       "3  Bundesgesundheitsblatt - Gesundheitsforschung ...  2007-05-01     358   \n",
       "4      Journal of the American College of Cardiology  2005-08-01     370   \n",
       "\n",
       "   micro_cluster                         main_field  \n",
       "0            241     Biomedical and health sciences  \n",
       "1            180  Physical sciences and engineering  \n",
       "2           1123     Biomedical and health sciences  \n",
       "3           3022     Biomedical and health sciences  \n",
       "4           1057     Biomedical and health sciences  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1966247273,\n",
       " 1992776590,\n",
       " 2054655287,\n",
       " 2097744687,\n",
       " 2127786259,\n",
       " 2161166547,\n",
       " 2169918686,\n",
       " 2607177050}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cited_by[245658]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 9 fields in line 131058, saw 15\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/thorwhalen/Dropbox/_odata/figiri/hcp/publications-hcp3__.tsv.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 9 fields in line 131058, saw 15\n"
     ]
    }
   ],
   "source": [
    "src = '/Users/thorwhalen/Dropbox/_odata/figiri/hcp/publications-hcp3__.tsv.zip'\n",
    "t = pd.read_csv(src, sep='\\t')\n",
    "print(f\"{t.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The k-d to 2-d problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After embedding your objects of interest into k-dimensional vectors, you might further embed them into two or three dimensions for visualization. This process inevitably loses some information and introduces distortions, but it's still valuable. The initial conversion of texts or images into numerical vectors also involved information loss, yet it was useful for your specific goals.\n",
    "\n",
    "Dimensionality reduction typically results in losing some details. The key is to preserve the essential information (the \"signal\") and minimize the loss of less important details (the \"noise\"). Done well, this can be beneficial, especially if it enhances the signal-to-noise ratio.\n",
    "\n",
    "This principle also applies to compressing vectors for visualization. The aim is to present complex data usefully and intuitively, enabling analysts to spot patterns and gain insights. It's a balance between maintaining utility and practical implementation when transforming k-dimensional data into two-dimensional forms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
