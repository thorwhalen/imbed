{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single word openai embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll compute openai embeddings for the most frequent English words and uplaod them to a github repository in various formats (which we will compare): csv, zipped csv, pickle, and possibly recode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = lambda seq, size: (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "from oa import openai\n",
    "\n",
    "def oa_embeddings(terms, engine='text-embedding-ada-002'):\n",
    "    if isinstance(terms, str):\n",
    "        terms = [terms]\n",
    "    responses = openai.Embedding.create(input=terms, engine=engine)\n",
    "    embeddings = {term: response['embedding'] for term, response in zip(terms, responses['data'])}\n",
    "    return embeddings\n",
    "\n",
    "from py2store import PickleStore\n",
    "\n",
    "def get_and_save_embeddings(terms, save_store= PickleStore('.'), chk_size=2000):\n",
    "    save_store = PickleStore('.')\n",
    "\n",
    "    from lkj import print_progress\n",
    "\n",
    "    chk_size = 2000\n",
    "    n = int(len(terms) / chk_size)\n",
    "    for i, chk in enumerate(chunker(list(terms), chk_size)):\n",
    "        print_progress(f\"({i}/{n})\")\n",
    "        new_embeddings = oa_embeddings(chk)\n",
    "        first = i * chk_size\n",
    "        last = (i + 1) * chk_size\n",
    "        save_store[f'openai_word_embeddings_{first}_{last}.pkl'] = new_embeddings\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'of', 'and', 'to', 'a', 'in', 'for', 'is', 'on', 'that']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from idiom import most_frequent_words\n",
    "\n",
    "words = most_frequent_words()\n",
    "list(words)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 23135851162),\n",
       " ('of', 13151942776),\n",
       " ('and', 12997637966),\n",
       " ('to', 12136980858),\n",
       " ('a', 9081174698),\n",
       " ('in', 8469404971),\n",
       " ('for', 5933321709),\n",
       " ('is', 4705743816),\n",
       " ('on', 3750423199),\n",
       " ('that', 3400031103)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(words.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_and_save_embeddings(words)\n",
    "# 3m41s to run on my 100_000 word list!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping, MutableMapping\n",
    "# go through all src data, chunked by 5, dict-merging the datas, and saving the result in targ \n",
    "def rechunk_and_save(\n",
    "        src: Mapping, \n",
    "        targ: MutableMapping, \n",
    "        *,\n",
    "        aggregate_chk_size = 5,\n",
    "        original_chk_size = 2000\n",
    "):\n",
    "    src_keys = list(src)\n",
    "    new_chunk_size = original_chk_size * aggregate_chk_size\n",
    "    for i, chk in enumerate(chunker(range(len(src)), aggregate_chk_size)):\n",
    "        d = dict()\n",
    "        for first, last in src_keys[slice(chk[0], chk[-1] + 1)]:\n",
    "            d.update(src[(first, last)])\n",
    "        targ[i * new_chunk_size, (i + 1) * new_chunk_size] = d\n",
    "\n",
    "\n",
    "from dol import KeyTemplate\n",
    "\n",
    "st = KeyTemplate(\n",
    "    'openai_word_embeddings_{first}_{last}.pkl', field_patterns=dict(first='[0-9]+', last='[0-9]+')\n",
    ")\n",
    "\n",
    "from py2store import PickleStore\n",
    "from dol import wrap_kvs, filt_iter, Pipe, cached_keys\n",
    "\n",
    "embeddings_wrapper = Pipe(\n",
    "    filt_iter(filt=lambda k: k.endswith('.pkl')),\n",
    "    wrap_kvs(\n",
    "        key_of_id=lambda x: tuple(map(int, st.str_to_tuple(x))), \n",
    "        id_of_key=st.tuple_to_str\n",
    "    ),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = embeddings_wrapper(PickleStore('./embeddings'))\n",
    "src = cached_keys(src, keys_cache=sorted)\n",
    "targ = embeddings_wrapper(PickleStore('./embeddings_2'))\n",
    "\n",
    "# rechunk_and_save(src, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dol import wrap_kvs\n",
    "\n",
    "wordvec_store = embeddings_wrapper(PickleStore('./embeddings'))\n",
    "wordvec = dict(**wordvec_store[0, 10000], **wordvec_store[10000, 20000], **wordvec_store[20000, 30000])\n",
    "# wordvec = wrap_kvs(wordvec, obj_of_data=np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"king\" and \"queen\" similarity: 0.9155\n",
      "\"king\" and \"man\" similarity: 0.8138\n",
      "\"queen\" and \"man\" similarity: 0.8259\n",
      "\"queen\" and \"woman\" similarity: 0.8799\n",
      "\"man\" and \"woman\" similarity: 0.9029\n",
      "\n",
      "New vector and \"king\" similarity: 0.9155\n",
      "New vector and \"man\" similarity: 0.6542\n",
      "New vector and \"woman\" similarity: 0.8342\n",
      "New vector and \"queen\" similarity: 0.8848\n",
      "\n",
      "\n",
      "\"paris\" and \"rome\" similarity: 0.8460\n",
      "\"paris\" and \"france\" similarity: 0.8926\n",
      "\"rome\" and \"france\" similarity: 0.8258\n",
      "\"rome\" and \"italy\" similarity: 0.8579\n",
      "\"france\" and \"italy\" similarity: 0.8816\n",
      "\n",
      "New vector and \"paris\" similarity: 0.8903\n",
      "New vector and \"france\" similarity: 0.7291\n",
      "New vector and \"italy\" similarity: 0.9006\n",
      "New vector and \"rome\" similarity: 0.8270\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Mapping\n",
    "\n",
    "Word2Vec = Mapping[str, list]\n",
    "\n",
    "def try_wordvec_arthimetic(\n",
    "        wordvec: Mapping,\n",
    "        obj_word_1 = 'king',\n",
    "        obj_word_2 = 'queen',\n",
    "        feature_word_1 = 'man',\n",
    "        feature_word_2 = 'woman',\n",
    "):\n",
    "    from scipy.spatial.distance import cosine\n",
    "    import numpy as np\n",
    "\n",
    "    def cosine_similarity(vec1, vec2):\n",
    "        return 1 - cosine(vec1, vec2)\n",
    "\n",
    "\n",
    "    print(f'\"{obj_word_1}\" and \"{obj_word_2}\" similarity: {cosine_similarity(wordvec[obj_word_1], wordvec[obj_word_2]):.4f}')\n",
    "    print(f'\"{obj_word_1}\" and \"{feature_word_1}\" similarity: {cosine_similarity(wordvec[obj_word_1], wordvec[feature_word_1]):.4f}')\n",
    "    print(f'\"{obj_word_2}\" and \"{feature_word_1}\" similarity: {cosine_similarity(wordvec[obj_word_2], wordvec[feature_word_1]):.4f}')\n",
    "    print(f'\"{obj_word_2}\" and \"{feature_word_2}\" similarity: {cosine_similarity(wordvec[obj_word_2], wordvec[feature_word_2]):.4f}')\n",
    "    print(f'\"{feature_word_1}\" and \"{feature_word_2}\" similarity: {cosine_similarity(wordvec[feature_word_1], wordvec[feature_word_2]):.4f}')\n",
    "    print(\"\")\n",
    "    v = np.array(wordvec[obj_word_1]) - np.array(wordvec[feature_word_1]) + np.array(wordvec[feature_word_2])\n",
    "\n",
    "    print(f'New vector and \"{obj_word_1}\" similarity: {cosine_similarity(v, wordvec[obj_word_1]):.4f}')\n",
    "    print(f'New vector and \"{feature_word_1}\" similarity: {cosine_similarity(v, wordvec[feature_word_1]):.4f}')\n",
    "    print(f'New vector and \"{feature_word_2}\" similarity: {cosine_similarity(v, wordvec[feature_word_2]):.4f}')\n",
    "    print(f'New vector and \"{obj_word_2}\" similarity: {cosine_similarity(v, wordvec[obj_word_2]):.4f}')\n",
    "    print(\"\\n\")\n",
    "\n",
    "try_wordvec_arthimetic(wordvec)\n",
    "try_wordvec_arthimetic(wordvec, 'paris', 'rome', 'france', 'italy')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(t['king'], t['queen'])=0.9154539883981325\n",
      "cosine_similarity(t['king'], t['man'])=0.8138182688759626\n",
      "cosine_similarity(t['queen'], t['man'])=0.8259629343785966\n",
      "cosine_similarity(t['queen'], t['woman'])=0.8798956693397046\n",
      "cosine_similarity(t['man'], t['woman'])=0.9029889578791404\n",
      "\n",
      "cosine_similarity(v, t['king'])=0.9156055782123844\n",
      "cosine_similarity(v, t['man'])=0.6543302273978167\n",
      "cosine_similarity(v, t['woman'])=0.8342070103171549\n",
      "cosine_similarity(v, t['queen'])=0.8848948703426839\n"
     ]
    }
   ],
   "source": [
    "from typing import Mapping\n",
    "\n",
    "Word2Vec = Mapping[str, list]\n",
    "\n",
    "def try_wordvec_arthimetic(t: Mapping):\n",
    "    from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "    def cosine_similarity(vec1, vec2):\n",
    "        return 1 - cosine(vec1, vec2)\n",
    "\n",
    "\n",
    "    print(f\"{cosine_similarity(t['king'], t['queen'])=}\")\n",
    "    print(f\"{cosine_similarity(t['king'], t['man'])=}\")\n",
    "    print(f\"{cosine_similarity(t['queen'], t['man'])=}\")\n",
    "    print(f\"{cosine_similarity(t['queen'], t['woman'])=}\")\n",
    "    print(f\"{cosine_similarity(t['man'], t['woman'])=}\")\n",
    "    print(\"\")\n",
    "    v = np.array(t['king']) - np.array(t['man']) + np.array(t['woman'])\n",
    "\n",
    "    print(f\"{cosine_similarity(v, t['king'])=}\")\n",
    "    print(f\"{cosine_similarity(v, t['man'])=}\")\n",
    "    print(f\"{cosine_similarity(v, t['woman'])=}\")\n",
    "    print(f\"{cosine_similarity(v, t['queen'])=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "def get_default_codecs():\n",
    "    codecs = {}\n",
    "\n",
    "    # Attempt to add gzipped pickle codec\n",
    "    try:\n",
    "        import pickle\n",
    "        import gzip\n",
    "        codecs['gzipped_pickle'] = {\n",
    "            'encoder': lambda f, d: pickle.dump(d, gzip.open(f, 'wb')),\n",
    "            'decoder': lambda f: pickle.load(gzip.open(f, 'rb'))\n",
    "        }\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    # Attempt to add numpy npz codec\n",
    "    try:\n",
    "        import numpy as np\n",
    "        codecs['numpy_npz'] = {\n",
    "            'encoder': lambda f, d: np.savez_compressed(f, d),\n",
    "            'decoder': lambda f: {k: v for k, v in np.load(f).items()}\n",
    "        }\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    # Attempt to add hdf5 codec\n",
    "    # try:\n",
    "    #     import h5py\n",
    "    #     def _hdf5_encode(data, filename):\n",
    "    #         with h5py.File(filename, 'w') as f:\n",
    "    #             for key, values in data.items():\n",
    "    #                 f.create_dataset(key, data=values, compression=\"gzip\")\n",
    "\n",
    "    #     def _hdf5_decode(filename):\n",
    "    #         with h5py.File(filename, 'r') as f:\n",
    "    #             return {key: f[key][()] for key in f.keys()}\n",
    "\n",
    "    #     codecs['hdf5'] = {\n",
    "    #         'encoder': lambda f, d: _hdf5_encode(d, f),\n",
    "    #         'decoder': lambda f: _hdf5_decode(f)\n",
    "    #     }\n",
    "    # except ImportError:\n",
    "    #     pass\n",
    "\n",
    "    # Attempt to add parquet codec\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        codecs['parquet'] = {\n",
    "            'encoder': lambda f, d: pd.DataFrame.from_dict(d, orient='index').transpose().to_parquet(f, compression='gzip'),\n",
    "            'decoder': lambda f: pd.read_parquet(f).to_dict(orient='list')\n",
    "        }\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    return codecs\n",
    "\n",
    "\n",
    "# You can then use this function in your benchmark_storage function\n",
    "# codecs = get_default_codecs()\n",
    "# results = benchmark_storage(data, codecs)\n",
    "\n",
    "\n",
    "def benchmark_storage(data, codecs=None, *, verbose=True):\n",
    "    from lkj import print_progress, clog\n",
    "    from functools import partial\n",
    "\n",
    "    _clog = partial(clog, verbose, log_func=print_progress)\n",
    "\n",
    "    if codecs is None:\n",
    "        codecs = get_default_codecs()\n",
    "\n",
    "    results = {}\n",
    "    for name, codec in codecs.items():\n",
    "        _clog(f'Benchmarking {name}')\n",
    "\n",
    "        filename = f'temp_{name}.data'\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            codec['encoder'](filename, data)\n",
    "            encoding_time = time.time() - start_time\n",
    "\n",
    "            encoded_size = os.path.getsize(filename)\n",
    "\n",
    "            start_time = time.time()\n",
    "            decoded_data = codec['decoder'](filename)\n",
    "            decoding_time = time.time() - start_time\n",
    "\n",
    "            decoded_size = sum([len(v) * 8 for v in decoded_data.values()])  # Assuming float64 (8 bytes per float)\n",
    "\n",
    "            results[name] = {\n",
    "                'decoded_n_bytes': decoded_size,\n",
    "                'encoded_n_bytes': encoded_size,\n",
    "                'encoding_time_in_seconds': encoding_time,\n",
    "                'decoding_time_in_seconds': decoding_time\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "            os.remove(filename)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# data = {\"key1\": [0.1, 0.2, ...], \"key2\": [1.1, 1.2, ...]}\n",
    "# results = benchmark_storage(data)\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gzipped_csv</th>\n",
       "      <th>gzipped_pickle</th>\n",
       "      <th>parquet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>decoded_n_bytes</th>\n",
       "      <td>1.310808e+06</td>\n",
       "      <td>1.310808e+06</td>\n",
       "      <td>1.310808e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>encoded_n_bytes</th>\n",
       "      <td>3.925119e+08</td>\n",
       "      <td>1.761167e+08</td>\n",
       "      <td>2.718476e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>encoding_time_in_seconds</th>\n",
       "      <td>1.388668e+02</td>\n",
       "      <td>2.779556e+02</td>\n",
       "      <td>3.537151e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decoding_time_in_seconds</th>\n",
       "      <td>8.359523e+00</td>\n",
       "      <td>2.618939e+00</td>\n",
       "      <td>5.103618e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           gzipped_csv  gzipped_pickle       parquet\n",
       "decoded_n_bytes           1.310808e+06    1.310808e+06  1.310808e+06\n",
       "encoded_n_bytes           3.925119e+08    1.761167e+08  2.718476e+08\n",
       "encoding_time_in_seconds  1.388668e+02    2.779556e+02  3.537151e+01\n",
       "decoding_time_in_seconds  8.359523e+00    2.618939e+00  5.103618e+00"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# results = benchmark_storage(wordvec)\n",
    "# pd.DataFrame.from_dict(results)\n",
    "\n",
    "# \t                        gzipped_csv\tgzipped_pickle\tparquet\n",
    "# decoded_n_bytes\t            1.310808e+06\t1.310808e+06\t1.310808e+06\n",
    "# encoded_n_bytes\t            3.925119e+08\t1.761167e+08\t2.718476e+08\n",
    "# encoding_time_in_seconds\t1.388668e+02\t2.779556e+02\t3.537151e+01\n",
    "# decoding_time_in_seconds\t8.359523e+00\t2.618939e+00\t5.103618e+00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dol import ValueCodecs, Pipe, Files, KeyTemplate\n",
    "\n",
    "# decoder here will unpickle data and remove remove the .pkl extension from the key\n",
    "src_wrap = Pipe(\n",
    "    KeyTemplate(\n",
    "        'openai_word_embeddings_{from_word:05.0f:\\d+}_{to_word:05.0f:\\d+}.pkl.gz',\n",
    "        from_str_funcs=dict(from_word=int, to_word=int),\n",
    "    ).key_codec(), \n",
    "    ValueCodecs.pickle() + ValueCodecs.gzip()\n",
    ")\n",
    "\n",
    "targ_wrap = Pipe(\n",
    "    KeyTemplate(\n",
    "        'openai_word_embeddings_{from_word:06.0f:\\d+}_{to_word:06.0f:\\d+}.pkl.gz',\n",
    "        from_str_funcs=dict(from_word=int, to_word=int),\n",
    "    ).key_codec(), \n",
    "    ValueCodecs.pickle() + ValueCodecs.gzip()\n",
    ")\n",
    "\n",
    "src = src_wrap(Files('./embeddings'))\n",
    "targ = targ_wrap(Files('./embeddings_2'))\n",
    "k, v = src.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Files(rootdir='./embeddings_2', subpath='', pattern_for_field=None, max_levels=None, include_hidden=False, assert_rootdir_existence=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thorwhalen/Dropbox/py/proj/o/slang/slang/util.py:95: RuntimeWarning: invalid value encountered in cast\n",
      "  snip_of_unichr_code = (nan * ones(unichr_code_of_snip.max() + 1)).astype(int)\n"
     ]
    }
   ],
   "source": [
    "from slang import fixed_step_chunker\n",
    "from functools import partial\n",
    "\n",
    "chk_size = 2500\n",
    "chunker = partial(fixed_step_chunker, chk_size=chk_size)\n",
    "for k in sorted(src):\n",
    "    v = src[k]\n",
    "    start_idx, _ = k\n",
    "    for i, chk in enumerate(chunker(v.items())):\n",
    "        new_start_idx = start_idx + i * chk_size\n",
    "        new_end_idx = new_start_idx + chk_size\n",
    "        targ[new_start_idx, new_end_idx] = dict(chk)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(90000, 100000),\n",
       " (0, 10000),\n",
       " (50000, 60000),\n",
       " (60000, 70000),\n",
       " (70000, 80000),\n",
       " (10000, 20000),\n",
       " (40000, 50000),\n",
       " (30000, 40000),\n",
       " (80000, 90000),\n",
       " (20000, 30000)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "t  = src[0, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Encoding', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_tiktoken', 'core', 'encoding_for_model', 'encoding_name_for_model', 'get_encoding', 'list_encoding_names', 'model', 'registry']\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "print(dir(tiktoken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6151, 856, 836, 374, 3276, 1965, 10663, 278]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "num_tokens('hi my name is anticonstitutional')\n",
    "\n",
    "\n",
    "tiktoken.encoding_for_model(GPT_MODEL).encode('hi my name is anticonstitutional')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1820, 387, 311, 315, 323]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken.encoding_for_model(GPT_MODEL).encode('the be to of and')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_core_bpe',\n",
       " '_encode_bytes',\n",
       " '_encode_only_native_bpe',\n",
       " '_encode_single_piece',\n",
       " '_mergeable_ranks',\n",
       " '_pat_str',\n",
       " '_special_tokens',\n",
       " 'decode',\n",
       " 'decode_batch',\n",
       " 'decode_bytes',\n",
       " 'decode_bytes_batch',\n",
       " 'decode_single_token_bytes',\n",
       " 'decode_tokens_bytes',\n",
       " 'decode_with_offsets',\n",
       " 'encode',\n",
       " 'encode_batch',\n",
       " 'encode_ordinary',\n",
       " 'encode_ordinary_batch',\n",
       " 'encode_single_token',\n",
       " 'encode_with_unstable',\n",
       " 'eot_token',\n",
       " 'max_token_value',\n",
       " 'n_vocab',\n",
       " 'name',\n",
       " 'special_tokens_set',\n",
       " 'token_byte_values']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tiktoken.encoding_for_model(GPT_MODEL)\n",
    "dir(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.max_token_value=100276, t.n_vocab=100277\n"
     ]
    }
   ],
   "source": [
    "print(f\"{t.max_token_value=}, {t.n_vocab=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~������'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.decode(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as',\n",
       " 'el',\n",
       " 'ct',\n",
       " 'nd',\n",
       " ' in',\n",
       " ' h',\n",
       " 'ent',\n",
       " 'id',\n",
       " ' n',\n",
       " 'am',\n",
       " '           ',\n",
       " ' to',\n",
       " ' re',\n",
       " '--',\n",
       " ' {',\n",
       " ' of',\n",
       " 'om',\n",
       " ');\\n',\n",
       " 'im',\n",
       " '\\r\\n',\n",
       " ' (',\n",
       " 'il',\n",
       " '//',\n",
       " ' and',\n",
       " 'ur',\n",
       " 'se',\n",
       " ' l',\n",
       " 'ex',\n",
       " ' S',\n",
       " 'ad',\n",
       " ' \"',\n",
       " 'ch',\n",
       " 'ut',\n",
       " 'if',\n",
       " '**',\n",
       " ' }',\n",
       " 'em',\n",
       " 'ol',\n",
       " '                ',\n",
       " 'th',\n",
       " ')\\n',\n",
       " ' {\\n',\n",
       " ' g',\n",
       " 'ig',\n",
       " 'iv',\n",
       " ',\\n',\n",
       " 'ce',\n",
       " 'od',\n",
       " ' v',\n",
       " 'ate',\n",
       " ' T',\n",
       " 'ag',\n",
       " 'ay',\n",
       " ' *',\n",
       " 'ot',\n",
       " 'us',\n",
       " ' C',\n",
       " ' st',\n",
       " ' I',\n",
       " 'un',\n",
       " 'ul',\n",
       " 'ue',\n",
       " ' A',\n",
       " 'ow',\n",
       " \" '\",\n",
       " 'ew',\n",
       " ' <',\n",
       " 'ation',\n",
       " '()',\n",
       " ' for',\n",
       " 'ab',\n",
       " 'ort',\n",
       " 'um',\n",
       " 'ame',\n",
       " ' is',\n",
       " 'pe',\n",
       " 'tr',\n",
       " 'ck',\n",
       " '�',\n",
       " ' y',\n",
       " 'ist',\n",
       " '----',\n",
       " '.\\n\\n',\n",
       " 'he',\n",
       " ' e',\n",
       " 'lo',\n",
       " ' M',\n",
       " ' be',\n",
       " 'ers',\n",
       " ' on',\n",
       " ' con',\n",
       " 'ap',\n",
       " 'ub',\n",
       " ' P',\n",
       " '               ',\n",
       " 'ass',\n",
       " 'int',\n",
       " '>\\n',\n",
       " 'ly',\n",
       " 'urn']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: t.decode([x]), (range(300, 400))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the be to of and'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.decode([1820, 387, 311, 315, 323])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
